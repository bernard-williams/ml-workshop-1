{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Workshop 1: General ML\n",
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and print some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All types as expected, sometimes mixing int and float but it doesn't matter so much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = [col for col, type_ in df.dtypes.items() if type_ in (np.int64, np.float64)]\n",
    "categorical = [col for col, type_ in df.dtypes.items() if col not in numerical]\n",
    "\n",
    "print(\"numerical\", numerical)\n",
    "print(\"categorical\", categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Missing data at columns \"Engine HP\", \"Engine Cylinders\", \"Number of Doors\"\n",
    "- max(Year) = 2017, dataset published on Dec 2016, some data for future\n",
    "- std > 0 for all columns => not constant columns\n",
    "- min(Engine Cylinders) = 0 => weird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't understand MPG, transform to l/100km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpg_to_l100km(mpg):\n",
    "    return 235.21 / mpg\n",
    "\n",
    "min_mpg = min(min(df[\"city mpg\"]), min(df[\"highway MPG\"]))\n",
    "max_mpg =  min(max(df[\"city mpg\"]), max(df[\"highway MPG\"]))\n",
    "\n",
    "print(\"Min {}mpg, {} l/100km\".format(min_mpg, mpg_to_l100km(min_mpg)))\n",
    "print(\"Max {}mpg, {} l/100km\".format(max_mpg, mpg_to_l100km(max_mpg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.66 l/100 km strange but might be OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in numerical:\n",
    "    ax = df[[col]].plot.hist(title=col + \" histogram\")\n",
    "    ax.set_xlabel(\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- year, Engine HP, MPG, MSRP - Not very nice distribution => data transformation might help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VALUES_TO_PLOT = 20\n",
    "\n",
    "for col in categorical:\n",
    "    value_counts = df[col].value_counts(normalize=True)\n",
    "    if len(value_counts) <= MAX_VALUES_TO_PLOT:\n",
    "        plt.figure()\n",
    "        ax = value_counts.plot(kind='bar')\n",
    "        ax.set_xlabel(\"Value\")\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(col + \" hisotgram\")\n",
    "    else:\n",
    "        print(\"=== {} ===\".format(col))\n",
    "        print(value_counts)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Market category\" - more values, we should split\n",
    "- \"Engine Fuel type\" - diesel => not a lot \n",
    "- \"Driven_wheels\" - all_wheel_drive and four_wheel_drive difference??? Maybe we can join to one category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last colums tells me the potencial of the signal for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the most correlating pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_THRESHOLD = 0.5\n",
    "\n",
    "most_correlated_pairs = []\n",
    "\n",
    "for i_col1, (col1, row) in enumerate(df_corr.items()):\n",
    "    for i_col2, (col2, val) in enumerate(row.items()):\n",
    "        \n",
    "        # avoid duplicity and same columns\n",
    "        if i_col1 <= i_col2:\n",
    "            continue\n",
    "            \n",
    "        if abs(val) > CORR_THRESHOLD:\n",
    "            most_correlated_pairs.append((col1, col2, val))\n",
    "            \n",
    "# Sort\n",
    "most_correlated_pairs = sorted(most_correlated_pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "most_correlated_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MPG correlation - not a surprise\n",
    "- Engine HP/cylinders - not a surprise\n",
    "- MSRP, Engine HP/Cylinders => Engine will probably be the best predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col1, col2, val in most_correlated_pairs:\n",
    "    df.plot.scatter(x=col1, y=col2, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- annomaly in Highway MPG => explore and correct/get rid of it\n",
    "- most graphs not very linear, data transformation might be good idea\n",
    "- over 1.5M too few examples => if it makes sense for the task get rid of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highway MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"highway MPG\"] > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1119,\"highway MPG\"] = 35  # Not using /= 10 due to repetitive script trigger\n",
    "df.loc[1119, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df[df[\"MSRP\"] > 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"MSRP\"] <= 1000000]\n",
    "print(df.shape)\n",
    "df[df[\"MSRP\"] > 1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to train/dev/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "splits = (0.6, 0.2, 0.2)\n",
    "datasets = {}\n",
    "\n",
    "train_dev, datasets[\"test\"] = train_test_split(df, test_size=splits[2], random_state=1)\n",
    "test_size = splits[1] / sum(splits[:2])  # to get 20% of the total, not 20% of  the 80%\n",
    "datasets[\"train\"], datasets[\"dev\"] = train_test_split(train_dev, test_size=test_size, random_state=1)\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(\"{} len={}, {}\".format(name, len(dataset), len(dataset)/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in datasets.items():\n",
    "    dataset.to_csv(\"data/data_clean_{}.csv\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
